{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e740b4d-aa46-4f9e-b860-81ae60534ac3",
     "showTitle": false,
     "title": ""
    },
    "id": "i5m7HwtsrNex"
   },
   "source": [
    "## Boilers for loading and running falcon-40b-instruct\n",
    "\n",
    "* Google Colab notebook information\n",
    "  * GPU: A100-SXM 40 GB\n",
    "  * Disk: 166.8 GB\n",
    "* Details on falcon-40b-instruct\n",
    "  * Documentation: [Hugging Face model card](https://huggingface.co/tiiuae/falcon-40b-instruct)\n",
    "  * Runtime latency: See below text generations for estimates\n",
    "  * Memory footprint: Roughly 25 GB of GPU RAM used with 4bit quantization\n",
    "  * License: [Apache 2.0](https://huggingface.co/tiiuae/falcon-40b-instruct#license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fc7220-82ea-45f4-b8f2-febf499b1d64",
     "showTitle": false,
     "title": ""
    },
    "id": "bz3OEcHXrNey"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd369caa-4965-4710-9611-b4b96da5c244",
     "showTitle": false,
     "title": ""
    },
    "id": "mAx4WnwarNey"
   },
   "outputs": [],
   "source": [
    "# detailed information on the GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c03627e-c94c-4cba-8678-9cdc2482364b",
     "showTitle": false,
     "title": ""
    },
    "id": "5ZOytoHzrNez"
   },
   "outputs": [],
   "source": [
    "# install necessary libraries\n",
    "\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "%pip install -q -U torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install -q -U einops==0.6.1\n",
    "%pip install -q -U bitsandbytes==0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c585315-960c-45ae-8f17-eee6c066e1f0",
     "showTitle": false,
     "title": ""
    },
    "id": "fCN8gVnErNe0"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d59cadf-9266-4d7e-986d-a0d3cc7ea62a",
     "showTitle": false,
     "title": ""
    },
    "id": "mjHuOa_JrNe0"
   },
   "outputs": [],
   "source": [
    "# print GPU available memory\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96Be11xZvkhu"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "\n",
    "transformers.set_seed(4129408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d69770-8ac6-417f-93ec-5a04e7624f24",
     "showTitle": false,
     "title": ""
    },
    "id": "Rl3EGKoXrNe1"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad52591-28e4-4a34-b71b-18b43cd4741e",
     "showTitle": false,
     "title": ""
    },
    "id": "he8VuAxrrNe1"
   },
   "outputs": [],
   "source": [
    "# load falcon-40b-instruct\" \n",
    "# see source: https://huggingface.co/tiiuae/falcon-40b-instruct#how-to-get-started-with-the-model\n",
    "\n",
    "# this cell will take a long time, to avoid: deploy the LLM as an API inference endpoint\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb267c9-c76b-4d56-a411-483841bb787a",
     "showTitle": false,
     "title": ""
    },
    "id": "6MxLSoPXrNe2"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "* For text generation options, refer to [https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline)\n",
    "* Below prompts are borrowed from [https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2W70_ee9e5za"
   },
   "outputs": [],
   "source": [
    "# custom text generation function\n",
    "# requires \"model\" and \"tokenizer\" global vars initiated above\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def generate_text_pipeline(\n",
    "    prompt: str, \n",
    "    max_new_tokens: int = 128, \n",
    "    do_sample: bool = True, \n",
    "    eos_token_ids: List[int] = tokenizer.eos_token_id\n",
    ") -> str:\n",
    "  \n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Args:\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        do_sample (bool, optional): Whether or not to use sampling. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    batch = tokenizer(\n",
    "        prompt,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    batch = batch.to('cuda')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(\n",
    "            inputs=batch.input_ids, \n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            eos_token_id=eos_token_ids,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_tokens[0][len(batch.input_ids[0]):], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    if generated_text[0] == \" \":\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhQuAUHOrNM5"
   },
   "source": [
    "### Example 1: Zero-shot reasoning conditioned on good performance\n",
    "* From https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222029ee-9eff-47c5-af04-0bcd222f1240",
     "showTitle": false,
     "title": ""
    },
    "id": "G1Pf1aWpSrnN"
   },
   "outputs": [],
   "source": [
    "# Example 1.1\n",
    "\n",
    "prompt = \"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{prompt}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bclu13KIrXN1"
   },
   "outputs": [],
   "source": [
    "# For reference, \n",
    "    # GPT-4 response: Half of the 16 balls are golf balls, which is 8 balls. Half of these golf balls are blue, so there are 4 blue golf balls. ✅ \n",
    "    # GPT-3.5 response: There are 4 blue golf balls. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res = generate_text_pipeline(prompt)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222029ee-9eff-47c5-af04-0bcd222f1240",
     "showTitle": false,
     "title": ""
    },
    "id": "EN4BicEYenJ0"
   },
   "outputs": [],
   "source": [
    "# Example 1.2\n",
    "\n",
    "prompt = \"Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{prompt}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvRK4GhPenJ0"
   },
   "outputs": [],
   "source": [
    "# For reference:\n",
    "    # GPT-4 response: No, it doesn't make logical sense because Daniel's barber does not work on Sundays. ✅\n",
    "    # GPT-3.5 response: No, it does not make logical sense for Daniel to go in for a haircut on Sunday because his barber works on Mondays, Wednesdays, and Fridays. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res = generate_text_pipeline(prompt)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res}\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VklIoMqnrmKa"
   },
   "source": [
    "### Example 2: Chain-of-thought reasoning with few-shot examples\n",
    "* From https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKZ1nyF3sDKG"
   },
   "outputs": [],
   "source": [
    "# Example 2.1\n",
    "\n",
    "prompt = \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{prompt}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2274bc1-98ee-414e-822f-b3f843d34795",
     "showTitle": false,
     "title": ""
    },
    "id": "9T-86g64SrnO"
   },
   "outputs": [],
   "source": [
    "# For reference, \n",
    "    # GPT 4 response: After using 20 apples for lunch, the cafeteria has 3 apples left. With the purchase of 6 more apples, they now have 9 apples. ✅\n",
    "    # GPT-3.5 response: The cafeteria now has 9 apples. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res = generate_text_pipeline(\n",
    "    prompt, \n",
    "    eos_token_ids = [tokenizer.eos_token_id , tokenizer.encode(\" Q: \")[0]]\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9r21uN_fWPL"
   },
   "outputs": [],
   "source": [
    "# Example 2.2\n",
    "\n",
    "prompt = \"Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. Q: How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. Q: How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{prompt}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2274bc1-98ee-414e-822f-b3f843d34795",
     "showTitle": false,
     "title": ""
    },
    "id": "3lafhphffWPU"
   },
   "outputs": [],
   "source": [
    "# For reference:\n",
    "    # GPT-4 response: Ben has 4 children and 50% of them are in college and no longer live at home. Therefore, the other 50%, which is 2 children, still live at home. ✅\n",
    "    # GPT-3.5 response: Two of Ben's children still live at home. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res = generate_text_pipeline(prompt)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res}\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkgbWXQRrv4h"
   },
   "source": [
    "### Example 3: Least to most prompting\n",
    "* From https://arxiv.org/abs/2205.10625\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkATjowRsaft"
   },
   "outputs": [],
   "source": [
    "# Example 3.1\n",
    "# Start with sub question #1\n",
    "\n",
    "sub_question_1 = \"It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\" \n",
    "f'Prompt: \"{sub_question_1}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db1162e-937d-4f4e-a437-dd658ebafea8",
     "showTitle": false,
     "title": ""
    },
    "id": "_bgCWI00SrnO"
   },
   "outputs": [],
   "source": [
    "# For reference, \n",
    "    # GPT 4 response: Sure, here are the steps: Amy takes 4 minutes to climb to the top of the slide. It takes her 1 minute to slide down. Therefore, each total trip (climbing up and sliding down) takes her 4 minutes (climbing) + 1 minute (sliding down) = 5 minutes. So, each trip takes Amy 5 minutes. ✅\n",
    "    # GPT-3.5 response: Sure! Let's break it down step by step. It takes Amy 4 minutes to climb to the top of the slide. It takes her 1 minute to slide down. So, each trip consists of climbing up and sliding down. The total time for each trip is 4 minutes (climbing up) + 1 minute (sliding down) = 5 minutes. Therefore, each trip takes 5 minutes. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res_1 = generate_text_pipeline(sub_question_1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res_1}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ksr763Ns60M"
   },
   "outputs": [],
   "source": [
    "# Now do sub question #2 by appending answer to sub question #1\n",
    "\n",
    "sub_question_2 = f\"{sub_question_1} {res_1} The slide closes in 15 minutes. Q: How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{sub_question_2}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deCs4BKomJI7"
   },
   "outputs": [],
   "source": [
    "# For reference:\n",
    "    # GPT-4 response: Sure, here are the steps: We've already established that each complete trip (climbing up and sliding down) takes Amy 5 minutes. The slide is open for 15 minutes. Therefore, the number of complete trips Amy can make is the total time the slide is open divided by the time each trip takes. That is, 15 minutes / 5 minutes per trip = 3 trips. So, Amy can slide down 3 times before the slide closes. ✅\n",
    "    # GPT-3.5 response: Sure! Let's break it down step by step. The slide closes in 15 minutes. Each trip takes 5 minutes (4 minutes to climb up + 1 minute to slide down). To determine how many trips Amy can take before the slide closes, we divide the total time available (15 minutes) by the time for each trip (5 minutes). 15 minutes / 5 minutes = 3 trips. Therefore, Amy can slide down the slide 3 times before it closes. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res_2 = generate_text_pipeline(sub_question_2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res_2}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw_-dUFLdt21"
   },
   "outputs": [],
   "source": [
    "# Example 3.2\n",
    "# Start with sub question #1\n",
    "\n",
    "sub_question_1 = \"It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\" \n",
    "f'Prompt: \"{sub_question_1}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db1162e-937d-4f4e-a437-dd658ebafea8",
     "showTitle": false,
     "title": ""
    },
    "id": "QYZs2_k_dt21"
   },
   "outputs": [],
   "source": [
    "# For reference:\n",
    "    # GPT-4 response: It takes Ben a total of 14 minutes (10 minutes driving + 4 minutes parking) to start shopping. ✅\n",
    "    # GPT-3.5 response: It takes Ben 14 minutes before he can start shopping. ✅\n",
    "\n",
    "start_time = time.time()\n",
    "res_1 = generate_text_pipeline(sub_question_1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res_1}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRy1VyRCdt22"
   },
   "outputs": [],
   "source": [
    "# Now do sub question #2 by appending answer to sub question #1\n",
    "\n",
    "sub_question_2 = f\"{sub_question_1} {res_1} The store closes in an hour. Q: Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "f'Prompt: \"{sub_question_2}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zNqM79_dt22"
   },
   "outputs": [],
   "source": [
    "# For reference:\n",
    "    # GPT-4 response: \n",
    "    # GPT-3.5 response: \n",
    "\n",
    "start_time = time.time()\n",
    "res_2 = generate_text_pipeline(sub_question_2)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "f'Text generation: \"{res_2}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMpYzW4kn25N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hf_LLM_load_and_run_example_dolly7b",
   "widgets": {}
  },
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
