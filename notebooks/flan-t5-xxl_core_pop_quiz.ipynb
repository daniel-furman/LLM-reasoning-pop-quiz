{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7e740b4d-aa46-4f9e-b860-81ae60534ac3",
          "showTitle": false,
          "title": ""
        },
        "id": "i5m7HwtsrNex"
      },
      "source": [
        "# LLM reasoning pop quiz\n",
        "\n",
        "Do open-sourced LLMs have the reasoning prowess of their closed-sourced siblings?\n",
        "\n",
        "* Google Colab notebook information\n",
        "  * GPU: A100-SXM 40 GB\n",
        "  * Disk: 166.8 GB\n",
        "* Details on flan-t5-xxl\n",
        "  * Documentation: [Hugging Face model card](https://huggingface.co/google/flan-t5-xxl)\n",
        "  * Latency: See below for examples with runtimes\n",
        "  * Memory footprint: Roughly  GB of GPU RAM used with fp16 precision\n",
        "  * License: [Apache 2.0](https://huggingface.co/google/flan-t5-xxl)\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/LLM-reasoning-pop-quiz/blob/main/notebooks/flan-t5-xxl_core_pop_quiz.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "63fc7220-82ea-45f4-b8f2-febf499b1d64",
          "showTitle": false,
          "title": ""
        },
        "id": "bz3OEcHXrNey"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fd369caa-4965-4710-9611-b4b96da5c244",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAx4WnwarNey",
        "outputId": "8d7a9d9d-5d72-41bf-aeef-ce9ee8f8cc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  7 00:39:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# detailed information on the GPU\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Gh4WEcED95",
        "outputId": "d0c59da8-1505-4f6a-bfa6-68d5c77d1234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM-reasoning-pop-quiz'...\n",
            "remote: Enumerating objects: 308, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 308 (delta 81), reused 102 (delta 43), pack-reused 165\u001b[K\n",
            "Receiving objects: 100% (308/308), 150.47 KiB | 1.98 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/daniel-furman/LLM-reasoning-pop-quiz.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdZGBvF_Jrxo",
        "outputId": "4fc74a49-ce38-47a4-bfd2-4cede5584447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM-reasoning-pop-quiz\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yigQDMrGGV1S",
        "outputId": "ffb60d15-5ecf-4120-b5ae-9b07a69c248c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for drf-llm-boilers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install necessary libraries\n",
        "import os\n",
        "\n",
        "os.chdir(\"/content/LLM-reasoning-pop-quiz\")\n",
        "!pip install -q -U -r requirements.txt\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3c585315-960c-45ae-8f17-eee6c066e1f0",
          "showTitle": false,
          "title": ""
        },
        "id": "fCN8gVnErNe0"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import time\n",
        "import yaml\n",
        "\n",
        "# import helpers\n",
        "\n",
        "from drf_llm_boilers import llm_boiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "glqF1V1RXgYp"
      },
      "outputs": [],
      "source": [
        "# set the seed\n",
        "\n",
        "transformers.set_seed(4129408)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3d59cadf-9266-4d7e-986d-a0d3cc7ea62a",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjHuOa_JrNe0",
        "outputId": "1c23c9fb-c9ba-488c-b791-75ff8edb924b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '37GB'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# print GPU available memory\n",
        "\n",
        "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
        "max_memory = f\"{free_in_GB-2}GB\"\n",
        "\n",
        "n_gpus = torch.cuda.device_count()\n",
        "max_memory = {i: max_memory for i in range(n_gpus)}\n",
        "max_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGyCV_6PED97"
      },
      "source": [
        "## Read in the yaml config for the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQtDrkjEED97",
        "outputId": "cd73c2d9-19f5-4ab8-93fc-4d6ec9d60f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompts': {'zero_shot': [\"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\",\n",
              "   \"Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"],\n",
              "  'cot_few_shot': [\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\",\n",
              "   \"Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"],\n",
              "  'least_to_most': [[\"Q: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\",\n",
              "    \"Q: The slide closes in 15 minutes. How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"],\n",
              "   [\"Q: It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\",\n",
              "    \"Q: The store closes in an hour. Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"]],\n",
              "  'tab_cot': [['Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\\n|step|subquestion|procedure|result|\\n|:---|:----------|:--------|:-----:|',\n",
              "    '\\nTherefore, the answer is'],\n",
              "   ['Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\\n|step|subquestion|procedure|result|\\n|:---|:----------|:--------|:-----:|\\n|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\\n|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\\n|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \\nTherefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  Jackson is planting tulips. He can fit 3 red tulips in a row and 4 blue tulips in a row. If Jackson buys 18 red tulips and 12 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\\n|step|subquestion|procedure|result|\\n|:---|:----------|:--------|:-----:|',\n",
              "    '\\nTherefore, the answer is'],\n",
              "   ['Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\\n|step|subquestion|procedure|result|\\n|:---|:----------|:--------|:-----:|\\n|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\\n|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\\n|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \\nTherefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the morning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did they have left?\\n|step|subquestion|procedure|result|\\n|:---|:----------|:--------|:-----:|',\n",
              "    '\\nTherefore, the answer is']]}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "with open(\n",
        "    \"/content/LLM-reasoning-pop-quiz/configs/pop_quiz_qa_style_v1.yml\", \"r\"\n",
        ") as file:\n",
        "    pop_quiz = yaml.safe_load(file)\n",
        "pop_quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "51d69770-8ac6-417f-93ec-5a04e7624f24",
          "showTitle": false,
          "title": ""
        },
        "id": "Rl3EGKoXrNe1"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "8d2ae5ddd79a47a586bc995148ed9556",
            "444acf645ae440c9817207b5c423d37c",
            "75edb5128fb64966842622f3c4687ea4",
            "1ec72641e80247e1b6962b688dc1ef48",
            "190f5580628f4de7a7fcdbbdc7f9e2e7",
            "76208b1e0b5445c99cc39ba3b245ba33",
            "a0d05783713249b1a1e6d0cc25f758cb",
            "9719c4a8198e411aa5331f61b3496130",
            "8493fb11960a49b4b905fc6e4980e654",
            "4837f2f1426e4252b2a2e9a2cbcd642c",
            "5bd98edf23f64d74b63b0bb35a592171"
          ]
        },
        "id": "wJEHXeDeI0zU",
        "outputId": "41436164-ca1a-4ccd-f94a-46df41cbec49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load function recognized for google/flan-t5-xxl: flan_loader\n",
            "Run function recognized for google/flan-t5-xxl: flan\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d2ae5ddd79a47a586bc995148ed9556"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load google/flan-t5-xxl\n",
        "# see source: https://huggingface.co/google/flan-t5-xxl#usage\n",
        "\n",
        "# this cell will take a long time, to avoid: deploy the LLM as an API inference endpoint\n",
        "\n",
        "model_id = \"google/flan-t5-xxl\"\n",
        "\n",
        "model = llm_boiler(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7XJRdkMMCPR",
        "outputId": "0e72fb43-583a-411a-ea35-83ade94d2a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flan \n",
            "\n",
            "T5Tokenizer(name_or_path='google/flan-t5-xxl', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True) \n",
            "\n",
            "T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 4096)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 4096)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 64)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 4096)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 64)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
            "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32128, bias=False)\n",
            ") \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(model.name, \"\\n\")\n",
        "print(model.tokenizer, \"\\n\")\n",
        "print(model.model, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bcb267c9-c76b-4d56-a411-483841bb787a",
          "showTitle": false,
          "title": ""
        },
        "id": "6MxLSoPXrNe2"
      },
      "source": [
        "## Run the model\n",
        "\n",
        "* For text generation options, refer to [https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline)\n",
        "* Below prompts are borrowed from [https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQuAUHOrNM5"
      },
      "source": [
        "### Example 1: Zero-shot reasoning conditioned on good performance\n",
        "* From https://arxiv.org/abs/2205.11916"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pR8hmCkNF_U",
        "outputId": "02bc9bed-187a-4d0b-8a78-00da188f9081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 1.1\n",
            "Prompt: \"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>The juggler has 16 golf balls * 1 / 2 = 8 golf balls. There are 8 golf balls * 0.5 = 4 blue golf balls. The answer:  4.</s>\n",
            "--- 2.1005547046661377 seconds ---\n",
            "\n",
            "\n",
            "Text generations: \"The juggler has 16 golf balls * 1 / 2 = 8 golf balls. There are 8 golf balls * 0.5 = 4 blue golf balls. The answer: 4.\"\n",
            "\n",
            "\n",
            "Question 1.2\n",
            "Prompt: \"Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>Daniel has to wait until the following week to get his haircut. The answer:  no.</s>\n",
            "--- 1.029297113418579 seconds ---\n",
            "\n",
            "\n",
            "Text generations: \"Daniel has to wait until the following week to get his haircut. The answer: no.\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# run zero shot questions\n",
        "\n",
        "for itr, prompt in enumerate(pop_quiz[\"prompts\"][\"zero_shot\"]):\n",
        "    print(f\"Question 1.{itr+1}\")\n",
        "    print(f'Prompt: \"{prompt}\"\\n')\n",
        "    start_time = time.time()\n",
        "    generated_text = model.run(\n",
        "        prompt=prompt,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=1.0,\n",
        "        do_sample=True,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generations: \"{generated_text}\"\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VklIoMqnrmKa"
      },
      "source": [
        "### Example 2: Chain-of-thought reasoning with few-shot examples\n",
        "* From https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vHxpU-gOVjr",
        "outputId": "73d270b0-1e33-487f-b402-927456b2acb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 2.1\n",
            "Prompt: \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>The cafeteria had 23 - 20 = 3 apples left after making lunch. They bought 6 + 3 = 7 apples. The answer is  7.</s>\n",
            "--- 1.772247552871704 seconds ---\n",
            "\n",
            "\n",
            "Text generations: \"The cafeteria had 23 - 20 = 3 apples left after making lunch. They bought 6 + 3 = 7 apples. The answer is 7.\"\n",
            "\n",
            "\n",
            "Question 2.2\n",
            "Prompt: \"Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>Ben has 4 children and 50% of them are in college so that's 4 *.50 = 2 children. 2 children still live at home. The answer is  2.</s>\n",
            "--- 2.026167392730713 seconds ---\n",
            "\n",
            "\n",
            "Text generations: \"Ben has 4 children and 50% of them are in college so that's 4 *.50 = 2 children. 2 children still live at home. The answer is 2.\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# run cot few-shot questions\n",
        "\n",
        "for itr, prompt in enumerate(pop_quiz[\"prompts\"][\"cot_few_shot\"]):\n",
        "    print(f\"Question 2.{itr+1}\")\n",
        "    print(f'Prompt: \"{prompt}\"\\n')\n",
        "    start_time = time.time()\n",
        "    generated_text = model.run(\n",
        "        prompt=prompt,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.01,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generations: \"{generated_text}\"\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkgbWXQRrv4h"
      },
      "source": [
        "### Example 3: Least to most prompting\n",
        "* From https://arxiv.org/abs/2205.10625\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzkHy6-_O00Y",
        "outputId": "b0bf24bc-b66a-4fae-e3a9-53e4787bcbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 3.1\n",
            "Prompt: \"Q: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>It takes Amy 4 / 1 = 3 minutes to slide down. So each trip takes 3 / 2 = 2 minutes. The answer:  2.</s>\n",
            "--- 1.7430152893066406 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"It takes Amy 4 / 1 = 3 minutes to slide down. So each trip takes 3 / 2 = 2 minutes. The answer: 2.\"\n",
            "\n",
            "Prompt: \"Q: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. It takes Amy 4 / 1 = 3 minutes to slide down. So each trip takes 3 / 2 = 2 minutes. The answer: 2. Q: The slide closes in 15 minutes. How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>She has 15 / 60 = 3 minutes left to slide. She can slide 3 / 3 = 2 times. The answer:  2.</s>\n",
            "--- 1.6464455127716064 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"She has 15 / 60 = 3 minutes left to slide. She can slide 3 / 3 = 2 times. The answer: 2.\"\n",
            "\n",
            "Question 3.2\n",
            "Prompt: \"Q: It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>It takes Ben 10 + 4 = 14 minutes to start shopping. The answer:  14.</s>\n",
            "--- 1.0164024829864502 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"It takes Ben 10 + 4 = 14 minutes to start shopping. The answer: 14.\"\n",
            "\n",
            "Prompt: \"Q: It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. It takes Ben 10 + 4 = 14 minutes to start shopping. The answer: 14. Q: The store closes in an hour. Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
            "\n",
            "<pad>He has 60 minutes to get to the store. The answer:  60.</s>\n",
            "--- 0.924044132232666 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"He has 60 minutes to get to the store. The answer: 60.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# run least to most questions\n",
        "\n",
        "for itr, prompts in enumerate(pop_quiz[\"prompts\"][\"least_to_most\"]):\n",
        "    print(f\"Question 3.{itr+1}\")\n",
        "    # Start with sub question #1\n",
        "    sub_question_1 = prompts[0]\n",
        "    print(f'Prompt: \"{sub_question_1}\"\\n')\n",
        "\n",
        "    start_time = time.time()\n",
        "    res_1 = model.run(\n",
        "        prompt=sub_question_1,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.01,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generation: \"{res_1}\"\\n')\n",
        "\n",
        "    # Now do sub question #2 by appending answer to sub question #1\n",
        "    sub_question_2 = f\"{sub_question_1} {res_1} {prompts[1]}\"\n",
        "    print(f'Prompt: \"{sub_question_2}\"\\n')\n",
        "\n",
        "    start_time = time.time()\n",
        "    res_2 = model.run(\n",
        "        prompt=sub_question_2,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.01,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generation: \"{res_2}\"\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No9YFIU-lnjX"
      },
      "source": [
        "### Example 4: Tab-CoT\n",
        "\n",
        "* See https://arxiv.org/abs/2305.17812"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyEe9za5E6oE",
        "outputId": "c6decb48-28a6-41e0-8e07-30ea7d218dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 4.1\n",
            "Prompt: \"Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\"\n",
            "\n",
            "<pad>Step 1: How many red tulips will Jackson plant? Step 2: How many blue tulips will Jackson plant? Step 3: How many rows of flowers will Jackson  plant?</s>\n",
            "--- 7.039231300354004 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"Step 1: How many red tulips will Jackson plant? Step 2: How many blue tulips will Jackson plant? Step 3: How many rows of flowers will Jackson plant?\"\n",
            "\n",
            "Prompt: \"Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:| Step 1: How many red tulips will Jackson plant? Step 2: How many blue tulips will Jackson plant? Step 3: How many rows of flowers will Jackson plant? \n",
            "Therefore, the answer is\"\n",
            "\n",
            "<pad>36 red tulips / 6 red tulips / row = 6 red tulips / row. 24 blue tulips / 8 blue tulips / row = 3 blue tulips / row. 6 red tulips / row + 3 blue tulips / row = 9 rows of  flowers.</s>\n",
            "--- 4.104530572891235 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"36 red tulips / 6 red tulips / row = 6 red tulips / row. 24 blue tulips / 8 blue tulips / row = 3 blue tulips / row. 6 red tulips / row + 3 blue tulips / row = 9 rows of flowers.\"\n",
            "\n",
            "Question 4.2\n",
            "Prompt: \"Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\n",
            "|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\n",
            "|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\n",
            "|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \n",
            "Therefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  Jackson is planting tulips. He can fit 3 red tulips in a row and 4 blue tulips in a row. If Jackson buys 18 red tulips and 12 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\"\n",
            "\n",
            "<pad>|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (18) by the number of red tulips per row (3).|18 / 3 = 6 rows| |2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (12) by the number of blue tulips per row (4).|12 / 4 = 3 rows| |3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips to the number of rows of blue tulips.|6 + 3 = 9 rows|. Therefore, the answer is Jackson will plant 9 total rows of  flowers.</s>\n",
            "--- 8.97022819519043 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (18) by the number of red tulips per row (3).|18 / 3 = 6 rows| |2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (12) by the number of blue tulips per row (4).|12 / 4 = 3 rows| |3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips to the number of rows of blue tulips.|6 + 3 = 9 rows|. Therefore, the answer is Jackson will plant 9 total rows of flowers.\"\n",
            "\n",
            "Prompt: \"Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\n",
            "|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\n",
            "|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\n",
            "|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \n",
            "Therefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  Jackson is planting tulips. He can fit 3 red tulips in a row and 4 blue tulips in a row. If Jackson buys 18 red tulips and 12 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:| |1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (18) by the number of red tulips per row (3).|18 / 3 = 6 rows| |2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (12) by the number of blue tulips per row (4).|12 / 4 = 3 rows| |3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips to the number of rows of blue tulips.|6 + 3 = 9 rows|. Therefore, the answer is Jackson will plant 9 total rows of flowers. \n",
            "Therefore, the answer is\"\n",
            "\n",
            "<pad>|6 + 3 = 9 rows|. Therefore, the answer is Jackson will plant 9 total rows of  flowers.</s>\n",
            "--- 1.436919927597046 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"|6 + 3 = 9 rows|. Therefore, the answer is Jackson will plant 9 total rows of flowers.\"\n",
            "\n",
            "Question 4.3\n",
            "Prompt: \"Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\n",
            "|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\n",
            "|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\n",
            "|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \n",
            "Therefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the morning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did they have left?\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\"\n",
            "\n",
            "<pad>|1|Total sales were 93 + 39 = 120 loaves.|200 - 120 = 80 loaves were left.|. Therefore, the answer is  80.</s>\n",
            "--- 2.12713623046875 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"|1|Total sales were 93 + 39 = 120 loaves.|200 - 120 = 80 loaves were left.|. Therefore, the answer is 80.\"\n",
            "\n",
            "Prompt: \"Example 1: Jackson is planting tulips. He can fit 6 red tulips in a row and 8 blue tulips in a row. If Jackson buys 36 red tulips and 24 blue tulips, how many rows of flowers will he plant? Format the response as a completion of this table.\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:|\n",
            "|1|How many rows of red tulips will Jackson plant?|Divide the total number of red tulips (36) by the number of red tulips per row (6).|36 / 6 = 6 rows|\n",
            "|2|How many rows of blue tulips will Jackson plant?|Divide the total number of blue tulips (24) by the number of blue tulips per row (8).|24 / 8 = 3 rows|\n",
            "|3|How many total rows of flowers will Jackson plant?|Add the number of rows of red tulips (6) to the number of rows of blue tulips (3).|6 + 3 = 9 rows|. \n",
            "Therefore, the answer is Jackson will plant 9 total rows of flowers. Example 2:  The bakers at the Beverly Hills Bakery baked 200 loaves of bread on Monday morning. They sold 93 loaves in the morning and 39 loaves in the afternoon. A grocery store returned 6 unsold loaves. How many loaves of bread did they have left?\n",
            "|step|subquestion|procedure|result|\n",
            "|:---|:----------|:--------|:-----:| |1|Total sales were 93 + 39 = 120 loaves.|200 - 120 = 80 loaves were left.|. Therefore, the answer is 80. \n",
            "Therefore, the answer is\"\n",
            "\n",
            "<pad> 80.</s>\n",
            "--- 0.2745184898376465 seconds ---\n",
            "\n",
            "\n",
            "Text generation: \"80.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# run tab-cot questions\n",
        "\n",
        "for itr, prompts in enumerate(pop_quiz[\"prompts\"][\"tab_cot\"]):\n",
        "    print(f\"Question 4.{itr+1}\")\n",
        "    # Start with sub question #1\n",
        "    sub_question_1 = prompts[0]\n",
        "    print(f'Prompt: \"{sub_question_1}\"\\n')\n",
        "\n",
        "    start_time = time.time()\n",
        "    res_1 = model.run(\n",
        "        prompt=sub_question_1,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.01,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generation: \"{res_1}\"\\n')\n",
        "\n",
        "    # Now do sub question #2 by appending answer to sub question #1\n",
        "    sub_question_2 = f\"{sub_question_1} {res_1} {prompts[1]}\"\n",
        "    print(f'Prompt: \"{sub_question_2}\"\\n')\n",
        "\n",
        "    start_time = time.time()\n",
        "    res_2 = model.run(\n",
        "        prompt=sub_question_2,\n",
        "        eos_token_ids=model.tokenizer.eos_token_id,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.01,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    print(\"\\n\")\n",
        "    print(f'Text generation: \"{res_2}\"\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB7Zi75xP6v3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "hf_LLM_load_and_run_example_dolly7b",
      "widgets": {}
    },
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d2ae5ddd79a47a586bc995148ed9556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_444acf645ae440c9817207b5c423d37c",
              "IPY_MODEL_75edb5128fb64966842622f3c4687ea4",
              "IPY_MODEL_1ec72641e80247e1b6962b688dc1ef48"
            ],
            "layout": "IPY_MODEL_190f5580628f4de7a7fcdbbdc7f9e2e7"
          }
        },
        "444acf645ae440c9817207b5c423d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76208b1e0b5445c99cc39ba3b245ba33",
            "placeholder": "​",
            "style": "IPY_MODEL_a0d05783713249b1a1e6d0cc25f758cb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "75edb5128fb64966842622f3c4687ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9719c4a8198e411aa5331f61b3496130",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8493fb11960a49b4b905fc6e4980e654",
            "value": 5
          }
        },
        "1ec72641e80247e1b6962b688dc1ef48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4837f2f1426e4252b2a2e9a2cbcd642c",
            "placeholder": "​",
            "style": "IPY_MODEL_5bd98edf23f64d74b63b0bb35a592171",
            "value": " 5/5 [03:12&lt;00:00, 36.00s/it]"
          }
        },
        "190f5580628f4de7a7fcdbbdc7f9e2e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76208b1e0b5445c99cc39ba3b245ba33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d05783713249b1a1e6d0cc25f758cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9719c4a8198e411aa5331f61b3496130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8493fb11960a49b4b905fc6e4980e654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4837f2f1426e4252b2a2e9a2cbcd642c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd98edf23f64d74b63b0bb35a592171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}