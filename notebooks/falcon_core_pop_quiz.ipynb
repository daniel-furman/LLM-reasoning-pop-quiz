{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e740b4d-aa46-4f9e-b860-81ae60534ac3",
     "showTitle": false,
     "title": ""
    },
    "id": "i5m7HwtsrNex"
   },
   "source": [
    "# LLM reasoning pop quiz\n",
    "\n",
    "Do open-sourced LLMs have the reasoning prowess of their closed-sourced siblings?\n",
    "\n",
    "* Google Colab notebook information\n",
    "  * GPU: A100-SXM 40 GB\n",
    "  * Disk: 166.8 GB\n",
    "* Details on falcon-40b-instruct\n",
    "  * Documentation: [Hugging Face model card](https://huggingface.co/tiiuae/falcon-40b-instruct)\n",
    "  * Runtime latency: Insert total time through the *n* questions in the pop quiz\n",
    "  * Memory footprint: Roughly 25 GB of GPU RAM used with 4bit quantization\n",
    "  * License: [Apache 2.0](https://huggingface.co/tiiuae/falcon-40b-instruct#license)\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/daniel-furman/LLM-reasoning-pop-quiz/blob/main/notebooks/falcon_core_pop_quiz.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fc7220-82ea-45f4-b8f2-febf499b1d64",
     "showTitle": false,
     "title": ""
    },
    "id": "bz3OEcHXrNey"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd369caa-4965-4710-9611-b4b96da5c244",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAx4WnwarNey",
    "outputId": "ae377384-622d-4b88-fc57-dd07359cb9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "# detailed information on the GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0Gh4WEcED95",
    "outputId": "46cc93cd-c654-44f5-aabf-52fa700509bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLM-reasoning-pop-quiz'...\n",
      "remote: Enumerating objects: 83, done.\u001b[K\n",
      "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 83 (delta 28), reused 70 (delta 25), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (83/83), 26.13 KiB | 1.24 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/daniel-furman/LLM-reasoning-pop-quiz.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon_core_pop_quiz.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yigQDMrGGV1S",
    "outputId": "dd473e31-d77f-4b13-b8bb-75698947e3cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for drf-llm-boilers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries\n",
    "import os\n",
    "\n",
    "os.chdir('/content/LLM-reasoning-pop-quiz')\n",
    "!pip install -q -U -r requirements.txt\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c585315-960c-45ae-8f17-eee6c066e1f0",
     "showTitle": false,
     "title": ""
    },
    "id": "fCN8gVnErNe0"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "# import helpers\n",
    "\n",
    "# from drf_llm_boilers import llm_boiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "glqF1V1RXgYp"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "\n",
    "transformers.set_seed(4129408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d59cadf-9266-4d7e-986d-a0d3cc7ea62a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjHuOa_JrNe0",
    "outputId": "81f95ffb-21dc-49cc-9937-e7fb0cd6707c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '37GB'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print GPU available memory\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "max_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGyCV_6PED97"
   },
   "source": [
    "## Read in the yaml config for the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YQtDrkjEED97"
   },
   "outputs": [],
   "source": [
    "# forthcoming\n",
    "\n",
    "with open('../configs/pop_quiz_v0.yml', 'r') as file:\n",
    "    pop_quiz = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d69770-8ac6-417f-93ec-5a04e7624f24",
     "showTitle": false,
     "title": ""
    },
    "id": "Rl3EGKoXrNe1"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355,
     "referenced_widgets": [
      "c40511e11efe4b50bde01768ec192988",
      "050e2d18f6e9450fb3c2e8e3b8e32edf",
      "a46f52a66b474268b88932fa2603565b",
      "3befa3d83b1344db96cdebb4df6a3097",
      "a47eb464fbaf4d0586f33c98cc9dc42b",
      "455aa7ef1b4e49eba412e1e34c362822",
      "87ce7658849746c7ba25a1ed662df9df",
      "e76ac22e06064e95918d381ef6e30fe8",
      "a1a56e0fb4914592ab1f858445436315",
      "90bd9c3b4b964f3296696c5d5b7c9cb1",
      "70f2c87f6a4e4ae897b256b6e39fe757"
     ]
    },
    "id": "wJEHXeDeI0zU",
    "outputId": "a4bc3b6c-11bf-4336-d563-2caea94f8222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load function recognized for tiiuae/falcon-40b-instruct: falcon_loader\n",
      "Run function recognized for tiiuae/falcon-40b-instruct: falcon\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40511e11efe4b50bde01768ec192988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load falcon-40b-instruct\n",
    "# see source: https://huggingface.co/tiiuae/falcon-40b-instruct#how-to-get-started-with-the-model\n",
    "\n",
    "# this cell will take a long time, to avoid: deploy the LLM as an API inference endpoint\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "model = llm_boiler(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7XJRdkMMCPR",
    "outputId": "95ec66a7-a54e-48b4-b109-3aa66ac5e723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falcon \n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='tiiuae/falcon-40b-instruct', vocab_size=65024, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['>>TITLE<<', '>>ABSTRACT<<', '>>INTRODUCTION<<', '>>SUMMARY<<', '>>COMMENT<<', '>>ANSWER<<', '>>QUESTION<<', '>>DOMAIN<<', '>>PREFIX<<', '>>SUFFIX<<', '>>MIDDLE<<']}, clean_up_tokenization_spaces=True) \n",
      "\n",
      "RWForCausalLM(\n",
      "  (transformer): RWModel(\n",
      "    (word_embeddings): Embedding(65024, 8192)\n",
      "    (h): ModuleList(\n",
      "      (0-59): 60 x DecoderLayer(\n",
      "        (ln_attn): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_mlp): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): Attention(\n",
      "          (maybe_rotary): RotaryEmbedding()\n",
      "          (query_key_value): Linear4bit(in_features=8192, out_features=9216, bias=False)\n",
      "          (dense): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=8192, out_features=32768, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dense_4h_to_h): Linear4bit(in_features=32768, out_features=8192, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=65024, bias=False)\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.name, '\\n')\n",
    "print(model.tokenizer, '\\n')\n",
    "print(model.model, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb267c9-c76b-4d56-a411-483841bb787a",
     "showTitle": false,
     "title": ""
    },
    "id": "6MxLSoPXrNe2"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "* For text generation options, refer to [https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline)\n",
    "* Below prompts are borrowed from [https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhQuAUHOrNM5"
   },
   "source": [
    "### Example 1: Zero-shot reasoning conditioned on good performance\n",
    "* From https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_quiz['prompts']['zero_shot']\n",
    "pop_quiz['prompts']['cot_few_shot']\n",
    "pop_quiz['prompts']['least_to_most']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222029ee-9eff-47c5-af04-0bcd222f1240",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1Pf1aWpSrnN",
    "outputId": "c656ee16-961d-4fc8-c99c-1b5e9fc78709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\n",
      "There are 8 golf balls in total. Half of the golf balls are blue, so there are 4 blue golf balls.<|endoftext|>\n",
      "--- 23.680741548538208 seconds ---\n",
      "\n",
      "\n",
      "Text generations: \"\n",
      "There are 8 golf balls in total. Half of the golf balls are blue, so there are 4 blue golf balls.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1.1\n",
    "\n",
    "prompt = \"Q: A juggler has 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "generated_text = model.run(\n",
    "    prompt,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generations: \"{generated_text}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTkZmQlSVMHT",
    "outputId": "4315ed55-ccb8-4850-de01-4f2b012f734d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\n",
      "No, it does not make logical sense for Daniel to go in for a haircut on Sunday. His barber works on Mondays, Wednesdays, and Fridays, so Daniel should have waited until one of those days to get a haircut.<|endoftext|>\n",
      "--- 34.40725827217102 seconds ---\n",
      "\n",
      "\n",
      "Text generations: \"\n",
      "No, it does not make logical sense for Daniel to go in for a haircut on Sunday. His barber works on Mondays, Wednesdays, and Fridays, so Daniel should have waited until one of those days to get a haircut.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1.2\n",
    "\n",
    "prompt = \"Q: Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "generated_text = model.run(\n",
    "    prompt,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generations: \"{generated_text}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VklIoMqnrmKa"
   },
   "source": [
    "### Example 2: Chain-of-thought reasoning with few-shot examples\n",
    "* From https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PM-yHR6JVh9a",
    "outputId": "76d1fcfa-e94b-42d5-d5f9-c3c2cef079dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. The cafeteria started with 23 apples. They used 20 to make lunch, so they have 3 apples left. They bought 6 more, so they have 9 apples now. The answer is 9.<|endoftext|>\n",
      "--- 33.8258113861084 seconds ---\n",
      "\n",
      "\n",
      "Text generations: \"The cafeteria started with 23 apples. They used 20 to make lunch, so they have 3 apples left. They bought 6 more, so they have 9 apples now. The answer is 9.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2.1\n",
    "\n",
    "prompt = \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does have now? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "generated_text = model.run(\n",
    "    prompt,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generations: \"{generated_text}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAyxelQ4Vmif",
    "outputId": "26b83535-bea3-4584-f86d-83e1aad6eca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. Q: How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. Q: How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. Q: How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. Q: How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Ben has 4 children. 50% of his children are in college and no longer live at home. So, 2 of his children are in college. Therefore, 2 of his children still live at home. The answer is 2.<|endoftext|>\n",
      "--- 39.971532583236694 seconds ---\n",
      "\n",
      "\n",
      "Text generations: \"Ben has 4 children. 50% of his children are in college and no longer live at home. So, 2 of his children are in college. Therefore, 2 of his children still live at home. The answer is 2.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2.2\n",
    "\n",
    "prompt = \"Q: Roger has 3 children. Each of his kids invited 4 of their friends to come to the birthday party. All of the friends came to the party. Q: How many children are at the party? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. Roger has 3 children, each of whom came to the party. Each of them have 4 friends coming over. 3 * 4 = 12. So 12 of their friends came to the party. 12 + 3 = 15. So, there are 15 children at the party in total. The answer is 15. Q: Ben has 4 children. 50% of his kids are in college and no longer live at home. Q: How many of Ben's children still live at home? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{prompt}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "generated_text = model.run(\n",
    "    prompt,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generations: \"{generated_text}\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkgbWXQRrv4h"
   },
   "source": [
    "### Example 3: Least to most prompting\n",
    "* From https://arxiv.org/abs/2205.10625\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkATjowRsaft",
    "outputId": "d5541ba3-091c-4e82-f405-f83794847466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\n",
      "Amy climbs up the slide for 4 minutes, then slides down for 1 minute. Therefore, each trip takes 5 minutes.<|endoftext|>\n",
      "--- 21.209710121154785 seconds ---\n",
      "\n",
      "\n",
      "Text generation: \"\n",
      "Amy climbs up the slide for 4 minutes, then slides down for 1 minute. Therefore, each trip takes 5 minutes.\"\n",
      "\n",
      "Prompt: \"It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Amy climbs up the slide for 4 minutes, then slides down for 1 minute. Therefore, each trip takes 5 minutes. The slide closes in 15 minutes. Q: How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Amy climbs up the slide for 4 minutes, then slides down for 1 minute. Therefore, each trip takes 5 minutes. The slide closes in 15 minutes. Q: How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Amy can slide down the slide 3 times before it closes. (15 minutes / 5 minutes per trip = 3 trips)<|endoftext|>\n",
      "--- 24.5720374584198 seconds ---\n",
      "\n",
      "\n",
      "Text generation: \"\n",
      "Amy can slide down the slide 3 times before it closes. (15 minutes / 5 minutes per trip = 3 trips)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.1\n",
    "\n",
    "# Start with sub question #1\n",
    "sub_question_1 = \"It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. Q: How long does each trip take? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\" \n",
    "print(f'Prompt: \"{sub_question_1}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "res_1 = model.run(\n",
    "    sub_question_1,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generation: \"{res_1}\"\\n')\n",
    "\n",
    "# Now do sub question #2 by appending answer to sub question #1\n",
    "sub_question_2 = f\"{sub_question_1} {res_1} The slide closes in 15 minutes. Q: How many times can she slide before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{sub_question_2}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "res_2 = model.run(\n",
    "    sub_question_2,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generation: \"{res_2}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deCs4BKomJI7",
    "outputId": "f4539d08-ab98-4f1b-ffb9-42c6f27f6898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\n",
      "Ben takes 10 minutes to drive to the store. He then takes 4 minutes to find parking. Therefore, it takes Ben 14 minutes to get to the store and find parking. Once he finds parking, he can start shopping.<|endoftext|>\n",
      "--- 36.469444036483765 seconds ---\n",
      "\n",
      "\n",
      "Text generation: \"\n",
      "Ben takes 10 minutes to drive to the store. He then takes 4 minutes to find parking. Therefore, it takes Ben 14 minutes to get to the store and find parking. Once he finds parking, he can start shopping.\"\n",
      "\n",
      "Prompt: \"It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Ben takes 10 minutes to drive to the store. He then takes 4 minutes to find parking. Therefore, it takes Ben 14 minutes to get to the store and find parking. Once he finds parking, he can start shopping. The store closes in an hour. Q: Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
      "\n",
      "It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Ben takes 10 minutes to drive to the store. He then takes 4 minutes to find parking. Therefore, it takes Ben 14 minutes to get to the store and find parking. Once he finds parking, he can start shopping. The store closes in an hour. Q: Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question. \n",
      "Ben has 46 minutes to shop before the store closes. He can make it to the store before it closes.<|endoftext|>\n",
      "--- 19.45189118385315 seconds ---\n",
      "\n",
      "\n",
      "Text generation: \"\n",
      "Ben has 46 minutes to shop before the store closes. He can make it to the store before it closes.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.2\n",
    "\n",
    "# Start with sub question #1\n",
    "sub_question_1 = \"It takes Ben 10 minutes to drive to the store. It then takes him 4 minutes to find parking before he can start shopping. Q: How long before he can start shopping? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\" \n",
    "print(f'Prompt: \"{sub_question_1}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "res_1 = model.run(\n",
    "    sub_question_1,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generation: \"{res_1}\"\\n')\n",
    "\n",
    "# Now do sub question #2 by appending answer to sub question #1\n",
    "sub_question_2 = f\"{sub_question_1} {res_1} The store closes in an hour. Q: Can he make to the store before it closes? A: Let's work this out in a step by step way to be sure we have the right answer. Respond as succinctly as possible to answer the question.\"\n",
    "print(f'Prompt: \"{sub_question_2}\"\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "res_2 = model.run(\n",
    "    sub_question_2,\n",
    "    eos_token_ids=model.tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, \n",
    "    temperature=0.01,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    ")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"\\n\")\n",
    "print(f'Text generation: \"{res_2}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMpYzW4kn25N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hf_LLM_load_and_run_example_dolly7b",
   "widgets": {}
  },
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "050e2d18f6e9450fb3c2e8e3b8e32edf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_455aa7ef1b4e49eba412e1e34c362822",
      "placeholder": "​",
      "style": "IPY_MODEL_87ce7658849746c7ba25a1ed662df9df",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "3befa3d83b1344db96cdebb4df6a3097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90bd9c3b4b964f3296696c5d5b7c9cb1",
      "placeholder": "​",
      "style": "IPY_MODEL_70f2c87f6a4e4ae897b256b6e39fe757",
      "value": " 9/9 [06:05&lt;00:00, 36.07s/it]"
     }
    },
    "455aa7ef1b4e49eba412e1e34c362822": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f2c87f6a4e4ae897b256b6e39fe757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87ce7658849746c7ba25a1ed662df9df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90bd9c3b4b964f3296696c5d5b7c9cb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a56e0fb4914592ab1f858445436315": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a46f52a66b474268b88932fa2603565b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e76ac22e06064e95918d381ef6e30fe8",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a1a56e0fb4914592ab1f858445436315",
      "value": 9
     }
    },
    "a47eb464fbaf4d0586f33c98cc9dc42b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c40511e11efe4b50bde01768ec192988": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_050e2d18f6e9450fb3c2e8e3b8e32edf",
       "IPY_MODEL_a46f52a66b474268b88932fa2603565b",
       "IPY_MODEL_3befa3d83b1344db96cdebb4df6a3097"
      ],
      "layout": "IPY_MODEL_a47eb464fbaf4d0586f33c98cc9dc42b"
     }
    },
    "e76ac22e06064e95918d381ef6e30fe8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
