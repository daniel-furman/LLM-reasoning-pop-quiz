# LLM-reasoning-pop-quiz

[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://github.com/daniel-furman/Polyglot-or-Not/blob/main/LICENSE) 
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/) 
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) 

Do open-source LLMs have the reasoning prowess of ChatGPT?

---

| model                      | quiz score* |
|----------------------------|:-------------:|
| openai/gpt-4               |      9/9       |
| openai/gpt-3.5             |      9/9       |
| tiiuae/falcon-40b-instruct |      9/9       |

**Table 1**: Overall quiz scores

[Forthcoming]

**Table 2**: Individual question scores

[Forthcoming]

**Table 3**: Generated responses with scores



### TO DOs

* read in a yaml file with prompts to run!
* add two more examples per type, perhaps referencing the papers for the other examples
* add direct caching to csv in notebook, enable running chunks separately, caching
* include gpt-3.5 api in class 
* include dolly-v2-12b in class
* include falcon-chat in class
